#+OPTIONS: toc:nil H:4
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LaTeX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LaTeX_HEADER: \usepackage{amssymb,amsmath}
#+LaTeX_HEADER: \usepackage{fancyhdr} %For headers and footers
#+LaTeX_HEADER: \pagestyle{fancy} %For headers and footers
#+LaTeX_HEADER: \usepackage{lastpage} %For getting page x of y
#+LaTeX_HEADER: \usepackage{float} %Allows the figures to be positioned and formatted nicely
#+LaTeX_HEADER: \restylefloat{figure} %and this command
#+LaTeX_HEADER: \usepackage{hyperref}
#+LaTeX_HEADER: \hypersetup{urlcolor=blue}
#+LaTex_HEADER: \usepackage{titlesec}
#+LaTex_HEADER: \setcounter{secnumdepth}{4}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \setminted{frame=single,framesep=10pt}
#+LaTeX_HEADER: \chead{}
#+LaTeX_HEADER: \rhead{\today}
#+LaTeX_HEADER: \cfoot{}
#+LaTeX_HEADER: \rfoot{\thepage\ of \pageref{LastPage}}
#+LaTeX_HEADER: \usepackage[parfill]{parskip}
#+LaTeX_HEADER:\usepackage{subfig}
#+latex_header: \hypersetup{colorlinks=true,linkcolor=black, citecolor=black}
#+LATEX_HEADER_EXTRA:  \usepackage{framed}


#+BEGIN_SRC ipython :session :exports none
  from ci import *
#+END_SRC

#+RESULTS:
: # Out[43]:


* Averages

** tables
#+BEGIN_SRC ipython :session  :exports results :results output drawer :exports none
  from tabulate import tabulate
  import pandas as pd
  atts = ['length', 'width', 'depth', 'volume', 'length_depth_width']

  means = lambda df: df.groupby(['Sample Type', 'Wild/Domesticated'],
                          as_index=False)[atts].mean()

  print(tabulate(means(pd.concat([einkorn, emmer, barley])),
                 ['idx','Sample Type', 'Domestication'] + atts, 'orgtbl') )
#+END_SRC

#+RESULTS:
:RESULTS:
| idx | Sample Type    | Domestication |  length |   width |   depth |  volume | length_depth_width |
|-----+----------------+---------------+---------+---------+---------+---------+--------------------|
|   0 | H. spontaneum  | wild          |  7.6705 | 2.83964 | 1.77413 | 19.7661 |            39.1044 |
|   1 | H. vulgare     | domesticated  | 5.54881 | 3.16844 | 2.27528 | 20.5476 |            40.4507 |
|   2 | T. beoticum    | wild          | 5.67469 |  2.3575 | 1.52986 | 11.3988 |            21.0029 |
|   3 | T. dicoccoides | wild          | 6.34376 | 2.53366 | 1.95585 | 17.2728 |            33.1034 |
|   4 | T. dicoccum    | domesticated  | 6.36795 | 3.05924 | 2.59588 | 25.7611 |            51.0032 |
|   5 | T. monococcum  | domesticated  | 5.76834 | 2.93219 | 2.09178 | 17.4042 |            35.5078 |
:END:


#+BEGIN_SRC ipython :session :results raw drawer :exports results
  import seaborn as sns
  from sklearn import preprocessing
  plt.style.use('classic')
  plt.rcParams['figure.figsize'] = (6, 4)
  min_max_scaler = preprocessing.MinMaxScaler()

  nmlEinkorn = einkorn.copy(deep=True)
  for idx, a in enumerate(atts):
      x = np.reshape(np.array(nmlEinkorn[a]), (-1,1))
      x = min_max_scaler.fit_transform(x)
      nmlEinkorn[a] = x + idx
  ax = nmlEinkorn[atts].plot()

  ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[45]:
[[file:./obipy-resources/Ymri45.png]]
:END:

* IDEA  Regression

** Simple Reg
#+BEGIN_SRC ipython :session :results output  :exports results
  import statsmodels.api as sm

  target = pd.DataFrame(einkorn['Wild/Domesticated'], columns=['Wild/Domesticated'])
  target['Wild/Domesticated'] = target['Wild/Domesticated'].replace((target['Wild/Domesticated'].unique()[0],
                                                                       target['Wild/Domesticated'].unique()[1]),
                                                                      (1, 0))
  x = einkorn['volume']
  y = target['Wild/Domesticated']

  model = sm.OLS(y,x).fit()
  predictions = model.predict(x)

  print(model.summary())

#+END_SRC

#+RESULTS:
#+begin_example
                            OLS Regression Results
==============================================================================
Dep. Variable:      Wild/Domesticated   R-squared:                       0.800
Model:                            OLS   Adj. R-squared:                  0.800
Method:                 Least Squares   F-statistic:                     2998.
Date:                Mon, 13 Aug 2018   Prob (F-statistic):          4.55e-264
Time:                        08:36:37   Log-Likelihood:                -318.03
No. Observations:                 750   AIC:                             638.1
Df Residuals:                     749   BIC:                             642.7
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
volume         0.0460      0.001     54.755      0.000       0.044       0.048
==============================================================================
Omnibus:                      128.141   Durbin-Watson:                   0.272
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               75.849
Skew:                          -0.646   Prob(JB):                     3.39e-17
Kurtosis:                       2.130   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example

#+BEGIN_SRC ipython :session :results raw drawer :exports results
  sns.regplot(x,y);
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[47]:
[[file:./obipy-resources/zkzcu0.png]]
:END:


** Playing with constants
#+BEGIN_SRC ipython :session :results output  :exports results

  import statsmodels.api as sm

  target = pd.DataFrame(einkorn['Wild/Domesticated'], columns=['Wild/Domesticated'])
  target['Wild/Domesticated'] = target['Wild/Domesticated'].replace((target['Wild/Domesticated'].unique()[0],
                                                                                                 target['Wild/Domesticated'].unique()[1]),
                                                                                                (1, 0))
  x = einkorn['volume']
  y = target['Wild/Domesticated']
  #x = sm.add_constant(x) # beta_0


  model = sm.OLS(y,x).fit()
  predictions = model.predict(x)

  print(model.summary())

#+END_SRC

#+RESULTS:
#+begin_example
                            OLS Regression Results
==============================================================================
Dep. Variable:      Wild/Domesticated   R-squared:                       0.800
Model:                            OLS   Adj. R-squared:                  0.800
Method:                 Least Squares   F-statistic:                     2998.
Date:                Mon, 13 Aug 2018   Prob (F-statistic):          4.55e-264
Time:                        08:36:38   Log-Likelihood:                -318.03
No. Observations:                 750   AIC:                             638.1
Df Residuals:                     749   BIC:                             642.7
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
volume         0.0460      0.001     54.755      0.000       0.044       0.048
==============================================================================
Omnibus:                      128.141   Durbin-Watson:                   0.272
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               75.849
Skew:                          -0.646   Prob(JB):                     3.39e-17
Kurtosis:                       2.130   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example

** Potentially useful for modelling Ploidy/DomStatus

#+BEGIN_SRC ipython :session :results output  :exports results
  def aggregate_average_attribute(df, att):
      return df.groupby(['Sample name', 'Sample Type', 'Wild/Domesticated', 'Ploidy'],
                                    as_index=False)[att].mean()

  atts = ['length','width','depth','length_depth_width', 'surface_area','volume']
  df = aggregate_average_attribute(pd.concat([einkorn, emmer]), atts)


  target = pd.DataFrame(df[['Wild/Domesticated', 'Ploidy']], columns=['Wild/Domesticated', 'Ploidy'])
  target['Wild_Domesticated'] = target['Wild/Domesticated'].replace((target['Wild/Domesticated'].unique()[0],target['Wild/Domesticated'].unique()[1]),(0, 1))
  df['Wild_Domesticated'] = target['Wild_Domesticated']

  x = df[atts]
  y = target['Wild_Domesticated']
  x = sm.add_constant(x, prepend=False)
  model = sm.OLS(y,x).fit()
  predictions = model.predict(x)
  print(model.summary())

#+END_SRC

#+RESULTS:
#+begin_example
                            OLS Regression Results
==============================================================================
Dep. Variable:      Wild_Domesticated   R-squared:                       0.719
Model:                            OLS   Adj. R-squared:                  0.670
Method:                 Least Squares   F-statistic:                     14.53
Date:                Mon, 13 Aug 2018   Prob (F-statistic):           3.81e-08
Time:                        08:45:37   Log-Likelihood:                -2.6872
No. Observations:                  41   AIC:                             19.37
Df Residuals:                      34   BIC:                             31.37
Df Model:                           6
Covariance Type:            nonrobust
======================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
length                 0.5717      0.356      1.604      0.118      -0.153       1.296
width                  1.6170      0.482      3.356      0.002       0.638       2.596
depth                  1.8142      0.888      2.043      0.049       0.010       3.619
length_depth_width    -0.0579      0.077     -0.755      0.456      -0.214       0.098
surface_area           0.0031      0.020      0.158      0.876      -0.037       0.043
volume                -0.0570      0.108     -0.527      0.602      -0.277       0.163
const                 -8.0497      3.073     -2.620      0.013     -14.294      -1.805
==============================================================================
Omnibus:                        6.519   Durbin-Watson:                   1.090
Prob(Omnibus):                  0.038   Jarque-Bera (JB):                5.590
Skew:                          -0.890   Prob(JB):                       0.0611
Kurtosis:                       3.319   Cond. No.                     4.18e+03
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.18e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
#+end_example



** GLM for W/D
#+BEGIN_SRC ipython :session :results output :exports results
  import statsmodels.formula.api as smf

  model = smf.ols('Wild_Domesticated ~  length * depth  * width -1 ', data=df).fit()

  print(model.summary())
#+END_SRC

#+RESULTS:
#+begin_example
                            OLS Regression Results
==============================================================================
Dep. Variable:      Wild_Domesticated   R-squared:                       0.895
Model:                            OLS   Adj. R-squared:                  0.874
Method:                 Least Squares   F-statistic:                     41.53
Date:                Mon, 13 Aug 2018   Prob (F-statistic):           7.70e-15
Time:                        08:45:52   Log-Likelihood:                -1.7740
No. Observations:                  41   AIC:                             17.55
Df Residuals:                      34   BIC:                             29.54
Df Model:                           7
Covariance Type:            nonrobust
======================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
length                -0.0944      0.452     -0.209      0.836      -1.013       0.824
depth                 10.9157      7.297      1.496      0.144      -3.914      25.745
length:depth          -1.9321      1.257     -1.537      0.134      -4.486       0.622
width                 -7.9841      3.402     -2.347      0.025     -14.897      -1.071
length:width           1.3502      0.529      2.551      0.015       0.275       2.426
depth:width            0.0353      1.336      0.026      0.979      -2.680       2.750
length:depth:width     0.0616      0.223      0.276      0.784      -0.392       0.515
==============================================================================
Omnibus:                        1.763   Durbin-Watson:                   1.267
Prob(Omnibus):                  0.414   Jarque-Bera (JB):                1.665
Skew:                          -0.455   Prob(JB):                        0.435
Kurtosis:                       2.616   Cond. No.                     7.68e+03
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 7.68e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
#+end_example

#+BEGIN_SRC ipython :session :results raw drawer :exports results :exports results
  from statsmodels.graphics.api import abline_plot
  plt.rcParams['figure.figsize'] = (14, 3)
  fig, ax = plt.subplots(1)
  ypred = model.predict(x)
  ax.plot(np.arange(len(x)), ypred,  c='r', label='prediction', linestyle='--')
  ax.scatter(np.arange(len(x)),y, label='actual values', c='b')
  ax.legend(loc='upper left')
  #ax.set_xlim(0, len(x))
  #ax.set_ylim(-0.5,1.5)
  ax.set_title(r'$R^2$:{0}'.format(np.around(model.rsquared, 2)))
  labels = [item.get_text() for item in ax.get_yticklabels()]
  labels[2] = 'Wild Einkorn'
  labels[7] = 'Domesticated\nEinkorn'
  _ = ax.set_yticklabels(labels)
  _ = ax.set_xlim([-0.5,31.5])
  _ = ax.set_xlabel('Individual Spikes')
  _ = ax.set_xticks([])

  fig.savefig('../Figures/Suppl/Reg_Dom.png')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[58]:
[[file:./obipy-resources/hinHeV.png]]
:END:


** Regression for showing 3D usefulness

#+BEGIN_SRC ipython :session :results raw drawer :exports results :exports results
  import statsmodels.formula.api as smf
  plt.rcParams['figure.figsize'] = (14, 10)
  fig, axes = plt.subplots(2)

  x = df[atts]
  y = df['volume']

  def plot_model(ax, model, title):
      ypred = model.predict(x)
      ax.plot(np.arange(len(x)), ypred,  c='r', label='prediction', linestyle='--')
      ax.scatter(np.arange(len(x)),y, label='actual values', c='b')
      sst_val = sum(map(lambda x: np.power(x,2),y-np.mean(y)))
      sse_val = sum(map(lambda x: np.power(x,2),model.resid_response))
      r2 = 1.0 - sse_val/sst_val
      ax.set_title('{1} | R2={0}'.format(np.around(r2,2), title))
      ax.set_ylabel('Volume')


  model1 = smf.glm('volume ~  length * depth * width -1', data=df).fit()
  model2 = smf.glm('volume ~  depth * width -1', data=df).fit()
  model3 = smf.glm('volume ~  length * width', data=df).fit()
  model4 = smf.glm('volume ~  length * depth', data=df).fit()

  plot_model(axes[0], model1, 'Length + Width + Depth')
  plot_model(axes[1], model3, 'Length + Width')
  fig.tight_layout()
  fig.savefig('../Figures/Suppl/Regression_Analysis_Vol.png')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[53]:
[[file:./obipy-resources/xSUFuD.png]]
:END:
